{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73831fcf-129f-4c4c-b68f-aad656c17572",
   "metadata": {},
   "source": [
    "# Baseline Model - Toxic Comment Classification\n",
    "\n",
    "* load data and preprocess\n",
    "* define train test split\n",
    "* define architecture and compile the model\n",
    "* train the model\n",
    "* evaluate the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02328628-21a4-4a3b-84a1-b61fc0f2ceca",
   "metadata": {},
   "source": [
    "## Multilabel Logistic Regression\n",
    "\n",
    "Multi-label classification assigns to each sample a set of target labels. Toxic comments can have one or multiple of the following labels:\n",
    "\n",
    "* toxic\n",
    "* severe_toxic\n",
    "* obscene\n",
    "* insult\n",
    "* identity_hate\n",
    "* threat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a520121-301f-4466-93d7-9111932d8d8b",
   "metadata": {},
   "source": [
    "### Import & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cee3b40-a97f-4f97-863a-85495501d675",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "\n",
    "from itertools import cycle\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, auc, confusion_matrix, classification_report, f1_score, roc_auc_score, roc_curve \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba858a1-69f8-4c3d-ad8f-76df971215fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "RSEED = 42\n",
    "TEST_SIZE = 0.33\n",
    "\n",
    "TRAIN_PATH = 'data/train.csv'\n",
    "TEST_PATH = 'data/test.csv'\n",
    "\n",
    "categories = ['toxic', 'severe_toxic', 'obscene', 'threat',  'insult', 'identity_hate']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cec50c-40df-4147-9831-7f73e97d1871",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "TODO: import preprocessing notebook and use its functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c11fc6-af97-4fcc-96ab-00b7473e6340",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    df = pd.read_csv(path)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b080e058-8d6d-4192-a12d-6ec0c179e612",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(X, Y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=TEST_SIZE, random_state=RSEED)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f74b33-99c2-487f-9c77-f8f4f7fa9a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic text cleaning\n",
    "# TODO: Replace with functions from preprocessing notebook\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"can not \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n",
    "    text = re.sub('\\W', ' ', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = text.strip(' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7e0b89-1011-4b1f-b082-00b2f77883b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df = load_data(TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47aca5c-10c8-4997-924f-bf5c9b84a828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean comment_text column\n",
    "df['comment_text'] = df['comment_text'].map(lambda com : clean_text(com))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15045b6a-d9f7-4811-9fbb-3aff9a825dd1",
   "metadata": {},
   "source": [
    "### Multi Label Logistic Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3931ca69-cb83-400d-9496-ea6dafe3939c",
   "metadata": {},
   "source": [
    "#### Setup and Train the Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7621cb02-a133-4f08-bdb0-00c1038ea8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split\n",
    "X = df['comment_text']\n",
    "Y = df[categories]\n",
    "X_train, X_test, y_train, y_test = data_split(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478fb15d-defa-428a-b263-94fa13441da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Logistic Regression Pipeline\n",
    "multi_label_clf = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
    "                ('clf', OneVsRestClassifier(LogisticRegression(solver='sag', random_state=RSEED), n_jobs=1)),\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff502141-f1b7-4aa5-9d0c-56ae178ee1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the classifier and make predictions\n",
    "multi_label_clf.fit(X_train, y_train)\n",
    "y_pred = multi_label_clf.predict(X_test)\n",
    "\n",
    "# Multilabel classification report\n",
    "\n",
    "f1_scores = f1_score(y_test, y_pred, average=None)\n",
    "avg_f1 = sum(f1_scores) / len(f1_scores)\n",
    "\n",
    "print(f\"Test Set average F1 Score: {avg_f1 * 100}%\\n\\n\")\n",
    "print(f\"Test Set Accuracy : {accuracy_score(y_test, y_pred) * 100}%\\n\\n\") \n",
    "print(f\"Classification Report : \\n\\n{classification_report(y_test, y_pred, zero_division=0)}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd5ee26-992e-47d0-861c-318cf429cf31",
   "metadata": {},
   "source": [
    "### Create submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b83a6c8-65b4-4ed9-b0da-86c56715abbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission_probabilities(clf, X_test):\n",
    "    for category in categories:\n",
    "        test_predictions = multi_label_clf.predict_proba(X_test)\n",
    "        submissions_df[category] = test_predictions[:, 1]\n",
    "        \n",
    "    submissions_df.to_csv('data/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bfd547-e154-4953-8f91-d61b71bb2ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Jigsaw Testset\n",
    "test_df = load_data(TEST_PATH)\n",
    "# Clean column with comments\n",
    "test_df['comment_text'] = test_df['comment_text'].map(lambda com : clean_text(com))\n",
    "\n",
    "# Define test data\n",
    "X_test_submission = test_df['comment_text']\n",
    "\n",
    "# Dataframe for predicted labels\n",
    "submissions_df = test_df['id'].to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef8f7d7-16ff-4d57-95b1-281116b73594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call if you want to calculate probabilities for kaggle test data and write them to submission.csv\n",
    "create_submission_probabilities(multi_label_clf, X_test_submission)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1bd220-8f6f-4e03-9d3d-d81aade83778",
   "metadata": {},
   "source": [
    "### Receiver Operating Characteristic (ROC)\n",
    "\n",
    "See also [sklearn documentation for ROC](https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4777fe72-eb03-4216-887a-046714b03d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# binarize the output\n",
    "y = label_binarize(Y, classes=[0, 1, 2, 3, 4, 5])\n",
    "n_classes = y.shape[1]\n",
    "\n",
    "n_samples, n_features = X.to_frame().shape\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = data_split(X, y)\n",
    "\n",
    "# Learn to predict each class against the other\n",
    "y_score = multi_label_clf.fit(X_train, y_train).decision_function(X_test)\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2b67b5-4532-4bd4-904d-fe16b80e3fda",
   "metadata": {},
   "source": [
    "#### Plot ROC curves for the multiclass problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d47bc5-21d1-4166-ab37-7d59d8976155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "# Plot all ROC curves\n",
    "plt.figure()\n",
    "\n",
    "colors = cycle([\"aqua\", \"darkorange\", \"cornflowerblue\"])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(\n",
    "        fpr[i],\n",
    "        tpr[i],\n",
    "        color=color,\n",
    "        lw=2,\n",
    "        label=\"ROC curve of class {0} (area = {1:0.5f})\".format(categories[i], roc_auc[i]),\n",
    "    )\n",
    "\n",
    "plt.plot([0, 1], [0, 1], \"k--\", lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"'Receiver Operating Characteristic')\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef77038-e3b6-4a40-81fa-cf0d0c93d6f8",
   "metadata": {},
   "source": [
    "#### Area under ROC for the multiclass problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e58f09-4833-4b9b-8226-dd4455de4735",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob = multi_label_clf.predict_proba(X_test)\n",
    "\n",
    "macro_roc_auc_ovr = roc_auc_score(y_test, y_prob, multi_class=\"ovr\", average=\"macro\")\n",
    "weighted_roc_auc_ovr = roc_auc_score(\n",
    "    y_test, y_prob, multi_class=\"ovr\", average=\"weighted\"\n",
    ")\n",
    "print(\n",
    "    \"One-vs-Rest ROC AUC scores:\\n{:.6f} (macro),\\n{:.6f} \"\n",
    "    \"(weighted by prevalence)\".format(macro_roc_auc_ovr, weighted_roc_auc_ovr)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
