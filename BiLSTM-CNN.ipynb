{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c41f50b0",
   "metadata": {},
   "source": [
    "# Modell test: BiLSTM-CNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "526ae8a7-f844-4536-9d95-c475c45997e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import optimizers\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras import metrics\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, Input, Convolution2D, MaxPooling2D, Concatenate\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Bidirectional, LSTM\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cef736b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(location_train, location_test):\n",
    "    df_train = pd.read_csv(location_train)\n",
    "    df_test = pd.read_csv(location_test)\n",
    "    df_train.lem_comments = df_train.lem_comments.astype(str)\n",
    "    df_test.lem_comments = df_test.lem_comments.astype(str)\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78861c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x_y_train(df_train):\n",
    "    x_train = df_train['lem_comments']\n",
    "    label_names = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "    y_train = df_train[label_names].values\n",
    "    return x_train, y_train, label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0566c3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x_test(df_train):\n",
    "    x_test = df_train['lem_comments']\n",
    "    return x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60eb41cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_y_test(location_y_test, label_names):\n",
    "    y_test = pd.read_csv(location_y_test)\n",
    "    col_names =['ids']\n",
    "    col_names = col_names + label_names\n",
    "    y_test.columns = col_names\n",
    "    return y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59bab3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(x_train, x_test):\n",
    "    raw_docs_train = x_train.tolist()\n",
    "    raw_docs_test = x_test.tolist()\n",
    "    tokenizer = Tokenizer(num_words=None, lower=True, char_level=False)\n",
    "    tokenizer.fit_on_texts(raw_docs_train)\n",
    "    tokenizer.fit_on_texts(raw_docs_test)\n",
    "    word_seq_train = tokenizer.texts_to_sequences(raw_docs_train)\n",
    "    word_seq_test = tokenizer.texts_to_sequences(raw_docs_test)\n",
    "    word_index = tokenizer.word_index\n",
    "    print(\"dictionary size: \", len(word_index))\n",
    "    return word_index, word_seq_train, word_seq_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85ba74ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(word_seq_train, word_seq_test):\n",
    "    max_seq_len = 168\n",
    "    word_seq_train = sequence.pad_sequences(word_seq_train, maxlen=max_seq_len)\n",
    "    word_seq_test = sequence.pad_sequences(word_seq_test, maxlen=max_seq_len)\n",
    "    return word_seq_train, word_seq_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0f525d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pipeline(location_train, location_test, location_y_test):\n",
    "    print('loading_data...')\n",
    "    df_train, df_test = load_data(location_train, location_test)\n",
    "    print('getting all the xs and ys..')\n",
    "    x_train, y_train, label_names = get_x_y_train(df_train)\n",
    "    x_test = get_x_test(df_train)\n",
    "    y_test = get_y_test(location_y_test, label_names)\n",
    "    print(\"tokenizing input data...\")\n",
    "    word_index, word_seq_train, word_seq_test = tokenize(x_train, x_test)\n",
    "    print('padding...')\n",
    "    word_seq_train, word_seq_test = padding(word_seq_train, word_seq_test)\n",
    "    return word_seq_train, y_train, word_seq_test, y_test, word_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b82fedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(location_embedding):\n",
    "    print('extracting word vextors...')\n",
    "    embeddings_index = {}\n",
    "    f = open(location_embedding, encoding='utf-8')\n",
    "    for line in tqdm(f):\n",
    "        values = line.rstrip().rsplit(' ')\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('preparing embedding matrix...')\n",
    "    words_not_found = []\n",
    "    embed_dim = 300\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, embed_dim))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= (len(word_index) + 1):\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        else:\n",
    "            words_not_found.append(word)\n",
    "    print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "    return embedding_matrix, words_not_found\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c67867e6-56d1-4ea6-ad03-ad429ed7baac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#data['doc_len'] = data['comment_text'].apply(lambda words: len(words.split(\" \")))\n",
    "#max_seq_len = np.round(data['doc_len'].mean() + data['doc_len'].std()).astype(int)\n",
    "\n",
    "max_seq_len = 168"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5f71ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading_data...\n",
      "getting all the xs and ys..\n",
      "tokenizing input data...\n",
      "dictionary size:  202005\n",
      "padding...\n",
      "extracting word vextors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000001it [00:59, 33552.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing embedding matrix...\n",
      "number of null word embeddings: 114000\n"
     ]
    }
   ],
   "source": [
    "# call functions to get started\n",
    "\n",
    "location_train = # add your path here\n",
    "location_test = # add your path here\n",
    "location_y_test = # add your path here\n",
    "location_embedding = # add your path here\n",
    "\n",
    "word_seq_train, y_train, word_seq_test, y_test, word_index = make_pipeline(location_train, location_test, location_y_test)\n",
    "embedding_matrix, words_not_found = get_embeddings(location_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e0c90f7-5236-43bd-b79a-e0321aefc1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 100000\n",
    "\n",
    "#training params\n",
    "batch_size = 256 \n",
    "num_epochs = 10 \n",
    "\n",
    "#model parameters\n",
    "num_filters = 64 \n",
    "embed_dim = 300 \n",
    "weight_decay = 1e-4\n",
    "\n",
    "input_dim = len(word_index) + 1\n",
    "#vocab_size = nb_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7cd0a9c-dbdd-44d5-9aa1-8ff3d23d2d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training BiLSTM-CNN ...\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 168)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding_3 (Embedding)        (None, 168, 300)     60601800    ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " bidirectional_3 (Bidirectional  (None, 168, 200)    320800      ['embedding_3[0][0]']            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv1d_9 (Conv1D)              (None, 162, 200)     280200      ['bidirectional_3[0][0]']        \n",
      "                                                                                                  \n",
      " conv1d_10 (Conv1D)             (None, 160, 200)     360200      ['bidirectional_3[0][0]']        \n",
      "                                                                                                  \n",
      " conv1d_11 (Conv1D)             (None, 158, 200)     440200      ['bidirectional_3[0][0]']        \n",
      "                                                                                                  \n",
      " max_pooling1d_9 (MaxPooling1D)  (None, 54, 200)     0           ['conv1d_9[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling1d_10 (MaxPooling1D  (None, 53, 200)     0           ['conv1d_10[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " max_pooling1d_11 (MaxPooling1D  (None, 52, 200)     0           ['conv1d_11[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " flatten_9 (Flatten)            (None, 10800)        0           ['max_pooling1d_9[0][0]']        \n",
      "                                                                                                  \n",
      " flatten_10 (Flatten)           (None, 10600)        0           ['max_pooling1d_10[0][0]']       \n",
      "                                                                                                  \n",
      " flatten_11 (Flatten)           (None, 10400)        0           ['max_pooling1d_11[0][0]']       \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 31800)        0           ['flatten_9[0][0]',              \n",
      "                                                                  'flatten_10[0][0]',             \n",
      "                                                                  'flatten_11[0][0]']             \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 256)          8141056     ['concatenate_3[0][0]']          \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 6)            1542        ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 70,145,798\n",
      "Trainable params: 9,543,998\n",
      "Non-trainable params: 60,601,800\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(\"training BiLSTM-CNN ...\")\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            300,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_seq_len,\n",
    "                            trainable=False)\n",
    "\n",
    "\n",
    "# inspiration: https://stackoverflow.com/questions/43150635/combining-the-outputs-of-multiple-models-into-one-model\n",
    "#parallel ip for different sections of image\n",
    "inp = Input(shape=(max_seq_len, ))\n",
    "emb = embedding_layer(inp)\n",
    "\n",
    "bilstm = Bidirectional(LSTM(100, return_sequences=True), merge_mode='concat')(emb)\n",
    "# paralle conv and pool layer which process the input with different kernels\n",
    "conv1 = Conv1D(200, 7, activation='relu')(bilstm)\n",
    "conv2 = Conv1D(200, 9, activation='relu')(bilstm)\n",
    "conv3 = Conv1D(200, 11, activation='relu')(bilstm)\n",
    "\n",
    "maxp1 = MaxPooling1D(3)(conv1)\n",
    "maxp2 = MaxPooling1D(3)(conv2)\n",
    "maxp3 = MaxPooling1D(3)(conv3)\n",
    "\n",
    "flt1 = Flatten()(maxp1)\n",
    "flt2 = Flatten()(maxp2)\n",
    "flt3 = Flatten()(maxp3)\n",
    "\n",
    "mrg = Concatenate()([flt1,flt2,flt3])\n",
    "\n",
    "dense = Dense(256, activation='relu')(mrg)\n",
    "\n",
    "op = Dense(6, activation='sigmoid')(dense)\n",
    "\n",
    "model = Model(inputs=inp, outputs=op)\n",
    "model.compile(optimizer='adam',\n",
    "              loss= 'binary_crossentropy',\n",
    "              metrics= metrics.AUC(),\n",
    "            )\n",
    "model.summary()              \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0c6177c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=21, verbose=1)\n",
    "callbacks_list = [early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0cc232d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-15 09:27:56.017218: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "624/624 [==============================] - 1112s 2s/step - loss: 0.0622 - auc: 0.9693\n",
      "Epoch 2/2\n",
      "624/624 [==============================] - 1177s 2s/step - loss: 0.0480 - auc: 0.9825\n"
     ]
    }
   ],
   "source": [
    "yay = model.fit(word_seq_train, y_train, epochs = 2, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ebf1cf17-7954-43af-9d5e-c2a5e0afb3a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(word_seq_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "78bb4633-dbf8-45c5-854d-5c01b44bcc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(len(y_pred)):    \n",
    "    for i in range(len(y_pred[1])):\n",
    "        if y_pred[j][i] >= 0.5:\n",
    "            y_pred[j][i] = 1\n",
    "        else: \n",
    "            y_pred[j][i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e53ac786-cbfd-4b33-aee2-5116645ab8b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061382c6-6c29-4865-837d-4ac2d46cdb62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5d624b97-7587-4dfc-9073-36c67b885440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-vs-Rest ROC AUC scores:\n",
      "0.772453 (macro),\n",
      "0.843408 (weighted by prevalence)\n"
     ]
    }
   ],
   "source": [
    "macro_roc_auc_ovr = roc_auc_score(y_test, y_pred, multi_class=\"ovr\", average=\"macro\")\n",
    "weighted_roc_auc_ovr = roc_auc_score(\n",
    "    y_test, y_pred, multi_class=\"ovr\", average=\"weighted\"\n",
    ")\n",
    "print(\n",
    "    \"One-vs-Rest ROC AUC scores:\\n{:.6f} (macro),\\n{:.6f} \"\n",
    "    \"(weighted by prevalence)\".format(macro_roc_auc_ovr, weighted_roc_auc_ovr)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea131131-eae8-4e85-9966-150a9074a0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting confusion matrix\n",
    "\n",
    "f, axes = plt.subplots(2, 3, figsize=(25, 15))\n",
    "axes = axes.ravel()\n",
    "for i in range(6):\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix(y_test[:, i],\n",
    "                                                   y_pred[:, i]),\n",
    "                                  display_labels=[f'non {label_names[i]}', label_names[i]])#[0, i])\n",
    "    disp.plot(ax=axes[i], values_format='.4g')\n",
    "    disp.ax_.set_title(f'toxicity label:\\n {label_names[i]}', fontsize=20)\n",
    "    if i<3:\n",
    "        disp.ax_.set_xlabel('')\n",
    "    if i%3!=0:\n",
    "        disp.ax_.set_ylabel('')\n",
    "    disp.im_.colorbar.remove()\n",
    "\n",
    "plt.subplots_adjust(wspace=0.8, hspace=0.01)\n",
    "f.colorbar(disp.im_, ax=axes)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
