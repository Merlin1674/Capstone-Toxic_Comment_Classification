{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmentation of less common categories with forward and back translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the necessary libraries and the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#mport deep-translator\n",
    "from deep_translator import GoogleTranslator #, MyMemoryTranslator\n",
    "from sklearn.utils import shuffle\n",
    "import time\n",
    "import progressbar\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data set with comments to be augmented\n",
    "data = pd.read_csv('data/df_cleaned&added_trans.csv')\n",
    "# adding a columns for information about augmented or not\n",
    "#data[\"augmented\"] = 0\n",
    "#data.clean_comments = data.clean_comments.astype(str)\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reducing the data set to the less categories; uncomment if necessary\n",
    "#data_threat = data.query(\"threat==1 or severe_toxic==1 or identity_hate==1\")\n",
    "#data_threat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or if the new data set is in a different file\n",
    "data_threat = pd.read_csv(\"data/new_coms.csv\")\n",
    "# adding some missing columns\n",
    "data_threat[\"augmented\"] = 0\n",
    "data_threat[\"clean_comments_without_stop_w\"] = \"NaN\"\n",
    "data_threat[\"lem_comments\"] = \"NaN\"\n",
    "data_threat[\"stem_comments\"] = \"NaN\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation of comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)\n",
    "index = data_threat.index.tolist()\n",
    "index_max = data.index.max()\n",
    "print(data_threat.shape)\n",
    "print(\"----------\")\n",
    "batchsize = list(range(0, len(index), int(len(index)/20)))\n",
    "batchsize.append(data_threat.shape[0])\n",
    "#for k in tqdm(range(data_threat.shape[0])):\n",
    "bar = progressbar.ProgressBar(max_value=data_threat.shape[0])\n",
    "k = 18 # da 85% schon erledigt\n",
    "for j in range(k, len(batchsize)-1):\n",
    "    start = time.perf_counter()\n",
    "    k += 1\n",
    "    for i in index[batchsize[j]:batchsize[j+1]]:\n",
    "        inter = data_threat.loc[[i]]\n",
    "        # if additional dataset, otherwise comment the next line out\n",
    "        data = data.append(inter, ignore_index=True, verify_integrity=True)\n",
    "        original= inter.comment_text.iloc[0][:2000]\n",
    "        #translate to german\n",
    "        translated = GoogleTranslator(source='english', target='german').translate(original)  \n",
    "        time.sleep(random.random()*4)\n",
    "        #and back to english\n",
    "        translated = GoogleTranslator(source='german', target='english').translate(translated[:2000])\n",
    "        #Another translator\n",
    "        #translated = MyMemoryTranslator(source='german', target='english').translate(translated)\n",
    "        time.sleep(random.random()*4)\n",
    "        #check if backtranslated not identical to original comment\n",
    "        if translated != original:\n",
    "            index_max += 1\n",
    "            inter.index = [index_max]\n",
    "            inter.comment_text = translated \n",
    "            inter.clean_comments = translated\n",
    "            inter.id = \"NaN\"\n",
    "            inter.clean_comments_without_stop_w = \"NaN\"\n",
    "            inter.lem_comments = \"NaN\"\n",
    "            inter.stem_comments = \"NaN\"\n",
    "            inter[\"augmented\"] = 1\n",
    "            data = data.append(inter, ignore_index=True, verify_integrity=True)\n",
    "        bar.update(i)\n",
    "    end = time.perf_counter()    \n",
    "    print(\"=========\")\n",
    "    print(k,\":\",k*5,\"%\",\"5% in \", end-start,\" s\")\n",
    "    #print(inter)\n",
    "    data = shuffle(data)\n",
    "    data.to_csv('data/df_cleaned&added_trans.csv', index=False)\n",
    "print(data.shape)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fc679e74b5ef59b7fe66de8600d655532b8c163340507bbae93fcb8f8e16ab24"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit ('3.9.4')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
