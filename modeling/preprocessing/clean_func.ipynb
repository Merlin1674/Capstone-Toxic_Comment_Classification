{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84ccb32d-7965-4a4e-9ab2-cd8b1947b07d",
   "metadata": {},
   "source": [
    "# Function to perform pre-processing:\n",
    "* clean noise\n",
    "* remove stopwords\n",
    "* stemming \n",
    "* lemmatization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5209684e-491c-48d2-8b0b-e954f9091c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import functions and packages, necessary for clean function\n",
    "import re\n",
    "import spacy\n",
    "import nltk\n",
    "import pandas as pd \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60be3537-1c6d-4ef9-8836-c54933fe5408",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download stopwords corpuses from nltk and spacy\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from stop_words import get_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20213b43-0f58-4704-b82e-7a80ccf05f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import stemming packages\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.arlstem import ARLSTem\n",
    "from nltk.stem.arlstem2 import ARLSTem2\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc160dc3-a646-4a48-b9cc-546562eae480",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_text(text, stop_words, stop_words_param):\n",
    "    \"\"\"Function to clean the text from noise, with the option to remove stopwords\n",
    "    \"\"\"\n",
    "    # regex to remove all Non-Alpha Numeric and space\n",
    "    special_character_removal=re.compile(r'[^a-z\\d ]',re.IGNORECASE)\n",
    "    # regex to replace all numeric\n",
    "    replace_numbers=re.compile(r'\\d+',re.IGNORECASE)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"iâ€™m\", \"i am\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = replace_numbers.sub('', text)\n",
    "    text = str(text).replace(\"\\n\", \" \")\n",
    "    text = str(text).replace(\"   \", \" \")\n",
    "    text = special_character_removal.sub('',text)\n",
    "    #check for stop_words parameters: nltk or spacy bibliothek for stopwords\n",
    "    if stop_words_param =='nltk':\n",
    "        stop_words_corpus = set(stopwords.words('english'))\n",
    "    elif stop_words_param =='spacy':\n",
    "        stop_words_corpus = set(STOP_WORDS)\n",
    "    else:\n",
    "        stop_words_corpus = get_stop_words('english')\n",
    "     #check for stop_words= True or False, when True, then remove\n",
    "    if stop_words: \n",
    "        #split into list for processing\n",
    "        text = text.split()\n",
    "        #check for stopwords and remove\n",
    "        text = [word for word in text if not word in stop_words_corpus]\n",
    "        text = ' '.join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4befa5-297a-4a94-8b15-7b8c09b75d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataframe(df, stop_words=True, lemmatization=True, stem_words=True, write_to_file_param=True, stop_words_param='nltk', lem_param='wn', stem_words_param='lancaster', path='data/'):\n",
    "    \"\"\"Function performs cleaning for 'comment_text' column in dataframe df. \n",
    "    Returns dataframe with new columns with cleaned text (with or without stopwords) and new column after stemming and lemmatazation\n",
    "    Function operates with following parameters:\n",
    "    df - dataframe with column 'comment_text'\n",
    "    stop_words - boolean \"True\" or \"False\" (by default \"True\"). This parameter defines whether perform cleaning with (or without) stop-words removal \n",
    "    lemmatization - boolean \"True\" or \"False\" (by default \"True\"). This parameter defines whether perform cleaning with (or without) lemmatization\n",
    "    stem_words - boolean \"True\" or \"False\" (by default \"True\"). This parameter defines whether perform cleaning with (or without) stemming\n",
    "    write_to_file_param - boolean \"True\" or \"False\" (by default \"True\"). This parameter defines whether to save dataframe with cleaned comment to file or not\n",
    "    stop_words_param - parameter to define corpus(default:'nltk', 'spacy' or 'py'), according to which function deletes stop-words\n",
    "    path - directory, to which file will be saved\n",
    "    lem_param - parameter to define lemmatizer ((default:'wn')\n",
    "    stem_words_param - parameter to define stemmer (default:'lancaster', 'snow', 'arl')\n",
    "    \"\"\"\n",
    "    if stop_words:\n",
    "        df['clean_comments_without_stop_w'] = df['comment_text'].map(lambda text : clean_text(text, stop_words, stop_words_param))\n",
    "        col_name = 'clean_comments_without_stop_w'\n",
    "    else:\n",
    "        df['clean_comments'] = df['comment_text'].map(lambda text : clean_text(text, stop_words, stop_words_param))\n",
    "        col_name = 'clean_comments'\n",
    "    if lemmatization:\n",
    "        df['lem_comments'] = df[col_name].map(lambda text : lem_sentence(text, lem_param))\n",
    "    if stem_words:\n",
    "        df['stem_comments'] = df[col_name].map(lambda text : stem_sentence(text, stem_words_param))\n",
    "    if write_to_file_param:\n",
    "        df.to_csv(path + 'df_cleaned.csv', index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624f25aa-c1bb-4f7c-989b-5773419169ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_sentence(sentence, stem_words_param):\n",
    "    \"\"\"Stemming function\n",
    "    \"\"\"\n",
    "    token_words = word_tokenize(sentence)\n",
    "    stem_sentence=[]\n",
    "    if stem_words_param =='snow':\n",
    "        stem_parameter = SnowballStemmer(\"english\")\n",
    "    elif stem_words_param =='arl':\n",
    "        stem_parameter = ARLSTem2()\n",
    "    else:  \n",
    "        stem_parameter = LancasterStemmer()\n",
    "    for word in token_words:\n",
    "        stem_sentence.append(stem_parameter.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a2f003-849e-4a76-94cf-8c0c7d2689cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lem_sentence(sentence, lem_param):\n",
    "    \"\"\"Lemmatization function\n",
    "    \"\"\"\n",
    "    token_words = word_tokenize(sentence)\n",
    "    lem_sentence=[]\n",
    "    if lem_param =='wn':\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        for word in token_words:\n",
    "            lem_sentence.append(lemmatizer.lemmatize(word))\n",
    "            lem_sentence.append(\" \")\n",
    "    return \"\".join(lem_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0117b1cc-aca0-4f1d-ae7f-bb8f381ccc7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4552b68a-1173-4174-8b7e-30b85ca985e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e8b9af-fe5d-4de1-b151-91ad30269479",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
