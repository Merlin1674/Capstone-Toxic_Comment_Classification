{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d266103",
   "metadata": {},
   "source": [
    "# Loading required libraries and the cleaned data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "526ae8a7-f844-4536-9d95-c475c45997e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential #, load_model\n",
    "from keras.layers import Embedding, GlobalMaxPooling1D, Bidirectional, LSTM, Dense, Dropout, Conv1D\n",
    "from keras.callbacks import EarlyStopping, Callback\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "from keras import metrics\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73d408e-ce1c-4636-930d-277d5394c377",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv')\n",
    "data.lem_comments = data.lem_comments.astype(str)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9671f37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964a5d2b-a9b8-432d-98a0-ef9bb5d30fa8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Word embedding\n",
    "## friendly help from: https://www.kaggle.com/vsmolyakov/keras-cnn-with-fasttext-embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b98fd13-6e2d-4d9c-a26c-9f545ddc1219",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#load embeddings\n",
    "print('loading word embeddings...')\n",
    "embeddings_index = {}\n",
    "f = open('modeling/embeddings/crawl-300d-2M-subword.vec', encoding='utf-8')\n",
    "for line in tqdm(f):\n",
    "    values = line.rstrip().rsplit(' ')\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('found %s word vectors' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197e0ad8",
   "metadata": {},
   "source": [
    "## First look on data set and the distribution of comment lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67867e6-56d1-4ea6-ad03-ad429ed7baac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Splitting the comments in lists of words\n",
    "data['doc_len'] = data['comment_text'].apply(lambda words: len(words.split(\" \")))\n",
    "max_seq_len = np.round(data['doc_len'].mean() + data['doc_len'].std()).astype(int)\n",
    "# and plotting the lengths\n",
    "sns.distplot(data['doc_len'], hist=True, kde=True, color='b', label='doc len')\n",
    "plt.axvline(x=max_seq_len, color='k', linestyle='--', label='max len')\n",
    "plt.title('comment length'); plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305c1e8a",
   "metadata": {},
   "source": [
    "## Train test split for validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428add37-4f9e-4da5-b5a2-6a171e26ef45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some fixed parameters\n",
    "testsize = 0.3\n",
    "randomstate = 42\n",
    "# train-test-split\n",
    "x = data['lem_comments']\n",
    "\n",
    "label_names = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = data[label_names].values\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = testsize, random_state = randomstate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7546d15-2f4a-4e86-8d8b-fc2668e92d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_docs_train = x_train.tolist()\n",
    "raw_docs_test = x_test.tolist()\n",
    "num_classes = len(label_names)\n",
    "len(raw_docs_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc748108",
   "metadata": {},
   "source": [
    "## Tokenizing and padding the sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0230c9d4-c897-4715-b0c9-a7d00622a0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"tokenizing input data...\")\n",
    "tokenizer = Tokenizer(num_words=None, lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(raw_docs_train)\n",
    "tokenizer.fit_on_texts(raw_docs_test)\n",
    "word_seq_train = tokenizer.texts_to_sequences(raw_docs_train)\n",
    "word_seq_test = tokenizer.texts_to_sequences(raw_docs_test)\n",
    "word_index = tokenizer.word_index\n",
    "print(\"dictionary size: \", len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdcb9c0-78f6-4140-a1a5-e56b7362a5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pad sequences to create sequences of same length --> CRUCIAL\n",
    "word_seq_train = sequence.pad_sequences(word_seq_train, maxlen=max_seq_len)\n",
    "word_seq_test = sequence.pad_sequences(word_seq_test, maxlen=max_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5c7a9d",
   "metadata": {},
   "source": [
    "## Embedding data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d308bdd-9a97-445e-817d-c45cd6ecda15",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 300 \n",
    "#embedding matrix\n",
    "print('preparing embedding matrix...')\n",
    "words_not_found = []\n",
    "#nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, embed_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i >= (len(word_index) + 1):\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        words_not_found.append(word)\n",
    "print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "print(\"sample words not found: \", np.random.choice(words_not_found, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c08b05",
   "metadata": {},
   "source": [
    "## Predicting classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be329aff-d120-4baa-8205-ef915a0bce70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_class(word_seq_test, y_test, threshold = 0.5):\n",
    "    print(\"Predicting lables\")\n",
    "    y_pred = model.predict(word_seq_test)\n",
    "    # transferring probability predictions in classes\n",
    "    for j in range(len(y_pred)):    \n",
    "        for i in range(len(y_pred[1])):\n",
    "            if y_pred[j][i] >= threshold:\n",
    "                y_pred[j][i] = 1\n",
    "            else: \n",
    "                y_pred[j][i] = 0\n",
    "    y_pred = y_pred.astype(int)\n",
    "    metric_values(y_test, y_pred)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45560d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_values(y_test, y_pred):\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    macro_roc_auc_ovr = roc_auc_score(y_test, y_pred, multi_class=\"ovr\", average=\"macro\")\n",
    "    weighted_roc_auc_ovr = roc_auc_score(\n",
    "        y_test, y_pred, multi_class=\"ovr\", average=\"weighted\")\n",
    "        \n",
    "    print(\n",
    "        \"One-vs-Rest ROC AUC scores:\\n{:.6f} (macro),\\n{:.6f} \"\n",
    "        \"(weighted by prevalence)\".format(macro_roc_auc_ovr, weighted_roc_auc_ovr)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc193ae6",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cd0a9c-dbdd-44d5-9aa1-8ff3d23d2d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bi_lstm_model():\n",
    "    print(\"training CNN ...\")\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(len(word_index) + 1, embed_dim,\n",
    "              weights=[embedding_matrix], input_length=max_seq_len, trainable=True))\n",
    "    model.add(Bidirectional(LSTM(20, return_sequences=True), merge_mode='concat')) # biLSTM-Layer\n",
    "    model.add(SeqSelfAttention(attention_activation='sigmoid')) # Basic attention layer\n",
    "    # model.add(SeqSelfAttention( # local attention layer\n",
    "    #    attention_width=15,\n",
    "    #    attention_activation='sigmoid',\n",
    "    #    name='Attention',\n",
    "    #))\n",
    "    #model.add(SeqSelfAttention(attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL, # Mulitiplicative attention\n",
    "    #                        kernel_regularizer=regularizers.l2(lr), # with regularizer\n",
    "    #                        bias_regularizer=regularizers.l1(lr),\n",
    "    #                        attention_regularizer_weight=lr,\n",
    "    #                        name='Attention'))\n",
    "    \n",
    "    model.add(Dropout(rate=0.8))\n",
    "    model.add(Conv1D(64, 7, activation='relu', padding='same'))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(num_classes, activation='sigmoid'))  #multi-label (k-hot encoding)\n",
    "\n",
    "    adam = optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=adam, metrics=metrics.AUC())#['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53ac786-cbfd-4b33-aee2-5116645ab8b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function for saving every epoch of fitting\n",
    "class CustomSaver(Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}, how_often=1):\n",
    "        if epoch % how_often == 0:  # or save after some epoch, each k-th epoch etc.\n",
    "            self.model.save(\"modeling/models/model_1{}.hd5\".format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb88c5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, word_seq_train, y_train, num_epochs=2):\n",
    "    # create and use callback:\n",
    "    saver = CustomSaver()\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=11, verbose=1)\n",
    "    callbacks_list = [early_stopping, saver]\n",
    "\n",
    "    result = model.fit(word_seq_train, y_train, batch_size=256, epochs=num_epochs, \n",
    "                 callbacks=callbacks_list, validation_split=0.1, shuffle=True, verbose=4)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e948af79",
   "metadata": {},
   "source": [
    "# Main program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea131131-eae8-4e85-9966-150a9074a0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_bi_lstm_model()\n",
    "model.summary()\n",
    "results = train_model(model, word_seq_train, y_train, num_epochs=2)\n",
    "y_pred = pred_class(word_seq_test, y_test, threshold = 0.5)\n",
    "\n",
    "metric_values(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0f0431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell for loading existing model \n",
    "# model = load_model('modeling/models/model_3.hd5')\n",
    "# y_pred = pred_class(word_seq_test, threshold = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d49170c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting confusion matrices for each category\n",
    "f, axes = plt.subplots(2, 3, figsize=(25, 15))\n",
    "axes = axes.ravel()\n",
    "for i in range(6):\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix(y_test[:, i],\n",
    "                                                   y_pred[:, i]),\n",
    "                                  display_labels=[f'non {label_names[i]}', label_names[i]])#[0, i])\n",
    "    disp.plot(ax=axes[i], values_format='.4g')\n",
    "    disp.ax_.set_title(f'toxicity label:\\n {label_names[i]}', fontsize=20)\n",
    "    if i<3:\n",
    "        disp.ax_.set_xlabel('')\n",
    "    if i%3!=0:\n",
    "        disp.ax_.set_ylabel('')\n",
    "    disp.im_.colorbar.remove()\n",
    "\n",
    "plt.subplots_adjust(wspace=0.8, hspace=0.01)\n",
    "f.colorbar(disp.im_, ax=axes)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
